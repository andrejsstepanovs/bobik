---
prompts:
  default: system_prompt.txt
  # private: /full_path_to/private_prompt.txt
  # code: /full_path_to/code_prompt.txt
history:
  # enable to store all questions and answers to file. Always appends (for now).
  enabled: false
  file: /full_path_to/my_history.txt
agent:
  prompts:
    - default
    # - private
  temperature: 0
  name: Bobik
  max_tries: 3
  sleep_seconds_between_tries: 2
  agent_type: conversational-react-description
  max_iterations: 4
  tools_enabled: true
phrases:
  exit: ["q", "exit", "quit"]
  with_tools:
    - agent
    - tools
    - with_tools
    - with-tools
  no_tools:
    - simple
    - llm
    - no-tools
    - no_tools
    - no-agent
    - no_agent
pre-parsers:
  # clipboard tool will add your active clipboard text on top of the question. Make sure word `clipboard` is part of question for this to happen.
  # example question: "Summarize my clipboard"
  clipboard: { enabled: true }
  # each time time, and other time related words will be used, time will be added. This can be not what you want for some, specific cases. For example, see `code` model that is defined in this example. 
  # There using this pre-parser will not be ideal, as we would send to LLM something not related to code.
  time: { enabled: true }
tools:
  # enable them one by one while checking that everything works. Agent is able to use multiple tools before getting to final answer.
  wttr_weather: { enabled: true }
  wikipedia: { enabled: true }
  clear_memory: { enabled: true }
  date_time_tool: { enabled: true }
  input_switch: { enabled: true }
  available_models: { enabled: true }
  model_switch: { enabled: true }
  end_conversation: { enabled: true }
  enable_disable_tools: { enabled: true }
  output_switch: { enabled: true }
  bing_search: { enabled: false }
  bing_news: { enabled: false }
  google_search: { enabled: false }
  ics_calendar:
    enabled: false
    config_file: /full_path_to/my_calendar.yaml
user:
  location: Germany, Berlin
  name: Master
  # timezone: UTC # optional
models:
  # LOCAL PROVIDERS setup example
  ollama:
    provider: ollama
    model: nexusraven:latest
    synonyms: ["private", "secure"]
  lmstudio:
    provider: lm_studio
    # model value is not needed, but code requires something.
    model: current
    synonyms: ["local"]
    tools_enabled: false
  my_company:
    provider: openai_custom
    model: bedrock/mistral.mistral-large-2402-v1:0
    base_url: http://localhost:1234
    synonyms: ["local"]

  # EXTERNAL LLM SERVICES
  groq:
    provider: groq
    model: llama3-70b-8192
    synonyms: ["llama 70", "fastest", "free"]
  gpt3:
    provider: openai
    model: gpt-3.5-turbo
    temperature: 1.9
    synonyms: ["gpt 3", "gpt-3"]
  gpt4o:
    provider: openai
    model: gpt-4o
    synonyms: ["gpt 4o", "gpt 4 o", "newest", "latest"]
  gpt4:
    provider: openai
    model: gpt-4
    synonyms: ["gpt 4", "gpt-4"]
  mistral:
    provider: mistral
    model: mistral-large-latest
    # specific use case when we dont want our agent to be chat bot.
    agent_type: zero-shot-react-description
    synonyms: ["mixtral"]
  # check if gemini is allowed in your country.
  google:
    provider: google
    model: gemini-1.5-flash-latest
    synonyms: ["gemini"]

  # CUSTOM MODEL EXAMPLE
  code:
    # useful for single runs with no tool (no agent) functionality enabled. This will result in much faster boot and answers.
    # cool example that you can plug into bash script loop:
    # cat my_code.py > computer.py --once --quiet code light refactor following code > my_code.py
    # "code" <- this model definition (uses mistral with no tools + special code prompt)
    provider: mistral
    model: open-mixtral-8x22b
    tools_enabled: false
    # set your custom prompt. needs to be defined on top.
    prompts:
      - code

io_input:
  text:
    provider: text
  listen:
    synonyms: ["ispeak"]
    provider: deepgram
    model: nova-2
    punctuate: true
    language: en-US
    encoding: linear16
    channels: 1
    sample_rate: 16000
    endpointing: 300
    smart_format: true
io_output:
  write:
    provider: text
  speak:
    synonyms: ["talk"]
    provider: deepgram
    performance: some
    encoding: linear16
    sample_rate: 24000
    model: aura-helios-en
